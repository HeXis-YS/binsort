
binsort - sort files by binary similarity

Written by Timm S. Mueller <tmueller@neoscientists.org>
Licensed under the 3-clause BSD license, see COPYRIGHT

Scans the contents of a directory, sorts the files by binary
similarity, generates a filelist and prints the list to stdout.
A possible application is to pass the list to an archiving tool,
e.g.:

$ binsort <dir> | tar -T- --no-recursion -czf out.tar.gz

This can improve compression rates considerably, although sorting
is in no way optimized for a particular compression algorithm.

This is a research project combining threshold accepting,
shingleprinting, and excessive multithreading.

This program uses simhash by Bart Massey (in modified form), and
Tiny Mersenne Twister by Mutsuo Saito and Makoto Matsumoto. See
COPYRIGHT for the respective copyrights and licensing terms.

Usage: binsort [options] dir
Options:
  -o          Optimization level [1...1000], default: 20
  -t          Number of threads [1...128], default: 4
  -q          Quiet operation, no progress indicators
  -d          Do not include directories in the output list
  -h  --help  This help

Note: Results are not stable unless you specify -t 1.

Results are not stable with more than one thread because binsort
uses a heuristic optimization algorithm, which depends on large
numbers of high-quality pseudo random numbers. Each thread
operates on a number generator of its own, and if two threads
compete for an overlapping range of data points, one is locked out
and will compute a new pair of random numbers, hence their
unpredictability.

Memory consumption: Converges to the squared number of files, in 
bytes - please consider this before sorting e.g. 100.000 files. On
a 32bit architecture, this cannot work anyway.

Further notes:

Binsort performs three time-consuming substeps: Hashing, delta
calculation, and optimizing. All three have been parallelized and
can be performed by any number of threads simultaneously. 

Hashing has linear time complexity, but may be time-consuming
nevertheless. It is the only substep that may be I/O bound. Delta
calculation and optimizing have quadratic time complexity. 

The number of threads should roughly match the number of cores in
the CPU, but a few more do not hurt, as they can help to improve
the interleaving of I/O and hashing.

Hashing and delta calculation are completely parallelizable, and
do not suffer from increased overhead for synchronization. During
optimization, on the other hand, threads must compete for certain
fields in the same data structure. Luckily, locking can be reduced
to very short sections of code, and this task is parallalizable to
a large extent as well.
