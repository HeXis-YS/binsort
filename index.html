<html>
	<title>
		Binsort - Sort files by binary similarity
	</title>
	<body>
<pre>

Binsort - sort files by binary similarity

Copyright (c) 2011 by Timm S. M&uuml;ller
Licensed under the 3-clause BSD license, see <a href="COPYRIGHT">COPYRIGHT</a>

Scans the contents of a directory, groups the files by binary
similarity, generates a filelist and prints the list to stdout. A
possible application is to pass the list to an archiving tool, e.g.:

$ binsort &lt;dir&gt; | tar -T- --no-recursion -czf out.tar.gz

This can improve compression rates considerably, although sorting is
in no way optimized for a particular compression algorithm.

This is a research project combining threshold accepting,
shingleprinting, and excessive multithreading. It uses simhash by Bart
Massey (in modified form), and Tiny Mersenne Twister by Mutsuo Saito
and Makoto Matsumoto. See <a href="COPYRIGHT">COPYRIGHT</a> for the respective copyright
holders' licensing terms.

Usage: binsort [options] dir
Options:
  -o          Optimization level [1...1000], default: 20
  -t          Number of threads [1...128], default: 4
  -q          Quiet operation, no progress indicators
  -d          Do not include directories in the output list
  -h  --help  This help

Note: Results are not stable unless you specify -t 1.

Results are not stable with more than one thread because binsort uses
a heuristic optimization algorithm, which depends on large numbers of
high-quality pseudo random numbers. Each thread operates on a number
generator of its own, and if two threads compete for an overlapping
range of data points, one is locked out and will compute a new pair
of random numbers, hence their unpredictability.

Memory consumption: Converges to the squared number of files, in
bytes. Please consider this before sorting e.g. 100.000 files. On a
32bit architecture, this cannot work anyway.

Further notes:

Binsort performs three time-consuming substeps: Hashing, distance
calculation, and optimizing. All three have been parallelized and can
be performed by any number of threads simultaneously. 

Hashing has linear time complexity, but may be time-consuming
nevertheless. It is the only substep that may be I/O bound. Delta
calculation and optimizing have quadratic time complexity. 

The number of threads should roughly match the number of CPUs (or CPU
cores), but a few more do not hurt, as they can help to improve the
interleaving of I/O and hashing.

Hashing and distances calculation are completely parallelizable, and
do not suffer from increased overhead for synchronization. During
optimization, on the other hand, threads compete for certain fields in
the same data structure. Luckily, locking can be reduced to very short
sections of code, and this task is parallalizable to a large extent as
well.

Contact:
Timm S. M&uuml;ller &lt;tmueller@schulze-mueller.de&gt;

Homepage:
<a href="http://neoscientists.org/~tmueller/binsort/">http://neoscientists.org/~tmueller/binsort/</a>

Download:
<a href="http://neoscientists.org/~tmueller/binsort/download/">http://neoscientists.org/~tmueller/binsort/download/</a>

Source code repository:
<a href="http://hg.neoscientists.org/binsort/">http://hg.neoscientists.org/binsort/</a>

</pre>
	</body>
</html>
